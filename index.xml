<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Joseph Daws on Joseph Daws</title>
    <link>https://joedaws.github.io/</link>
    <description>Recent content in Joseph Daws on Joseph Daws</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 -0400</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>A Polynomial-Based Approach for Architectural Design and Learning with Deep Neural Networks</title>
      <link>https://joedaws.github.io/publication/polynomials-for-nets/</link>
      <pubDate>Tue, 28 May 2019 17:10:52 -0400</pubDate>
      
      <guid>https://joedaws.github.io/publication/polynomials-for-nets/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Polynomial Inspired Neural Network Design and Initiailzation</title>
      <link>https://joedaws.github.io/post/binary-tree-network/</link>
      <pubDate>Tue, 14 May 2019 11:36:57 -0500</pubDate>
      
      <guid>https://joedaws.github.io/post/binary-tree-network/</guid>
      <description>

&lt;h1 id=&#34;polynomial-approximation-can-inform-network-design&#34;&gt;Polynomial approximation can inform network design&lt;/h1&gt;

&lt;p&gt;The following is a breif discussion of work found in this &lt;a href=&#34;https://arxiv.org/abs/1905.10457&#34; target=&#34;_blank&#34;&gt;preprint&lt;/a&gt;
which has been submitted to NeurIPS 2019. In this post I&amp;rsquo;ve left out
many details. If you are interested in these details please see the
preprint. As described in my previous
&lt;a href=&#34;https://joedaws.github.io/post/designing-neural-networks/&#34; title=&#34;Neural Network Design&#34; target=&#34;_blank&#34;&gt;post&lt;/a&gt;
Neural networks have emerged as a very powerful tool for constructing
nonlinear functions in high-dimensions. They are known to be very expressive
in the sense that the class of functions they can approximate is very wide.
However, in order to cosntruct an approximation of a given function
a very large number of network parameters must be determined.
This is espcially true for Deep Neural Networks (DNNs).
Because DNNs have many parameters and because the functions
assocaited with solving tasks like image classification
many be quite complicated, training can be very difficult.
Although many aspects of neural network are new (such as fast training algorithms
implementable on a GPU) their expressive power and ability to appoximate
has been known for decades. However, there do not seem to be
many works which make explicit connections between the powerful
results of classical approximation theory to expressibility results for
neural networks. Below is an outline of one way in which polynomial approximation
may inform efficient approximation of complicated functions by networks.
The primary connection between these kinds of approximation is made by
by constructing a network which can approximate a given polynomial.
One can then take advantage of existing polynomial approximate results
by first finding a suitable polynomial approximation then constructing a network
which achieves this approximation. After it has been initialzed to behave
like a polynomial, the network can be trained. Our numerical examples show that
networks initialized by our proposed method have
better performance than the polynomial used to initialize them.&lt;/p&gt;

&lt;h2 id=&#34;a-network-for-approximating-polynomials&#34;&gt;A Network for Approximating Polynomials&lt;/h2&gt;

&lt;p&gt;Polynomials are natural objects of interest in many different areas of mathematics.
A generic degree $n$ polynomial $P_n$ is often written
$$ P_n(x) = a_0 + a_1 x + a_2 x^2 + \cdots + a_n x^n. $$
Written in this way the polynomial $P_n$ is completely characterized
by its coefficients $a_i$ for $i=0,&amp;hellip;,n$.
Therefore, finding a suitable polynomial approximation is equivalent to
identifying coefficients. Instead of writing the polynomial
as a sum of the powers of the input $x$ one may also write a
polynomial as the product of linear shifts of the input.
According to the &lt;strong&gt;Fundamental Theorem of Algebra&lt;/strong&gt;
every degree $n$ polynomial can be written
$$P_n(x) = a\Pi_{i=1}^n (x - r_i)$$
for $n$ complex numbers $r_i$ and where $a$ is a scaling factor.
We construct a network which approximates $d$-dimensional
generalizations of polynomials like $P$ by approximating the product
of $n$ numbers with a network. Such a network can be constructed
by simplying chaining together several instances of a network
which approximates the product of two numbers.
Noticing that for two real numbers $a$ and $b$
$$ 2ab = (a+b)^2 -a^2 - b^2$$, we can approximate a product by
a linear combination of the outputs of three networks. One
which approximtes $(a+b)^2$, one which approximates $a^2$ and
one which approximates $b^2$. A network which approximtaes the mapping
$x \mapsto x^2$ can be constructed from an $L$ layer network
with $4$ nodes per level.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;x2_net_new.png&#34; alt=&#34;Network for x2&#34; width=&#34;200&#34;/&gt;&lt;/p&gt;

&lt;p&gt;Chaining several of these networks together we can approximate
the product of $n$ numbers and hence a polynomial. The following is
a rough outline the structure of network which approximates a polynomial.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;polynomial_network.png&#34; alt=&#34;Polynomial network&#34; width=&#34;400&#34;/&gt;&lt;/p&gt;

&lt;h2 id=&#34;initialization-based-on-polynomials&#34;&gt;Initialization based on polynomials&lt;/h2&gt;

&lt;p&gt;It is possible to initialize a network to have polynomial behavior.
Below, we have initiailzed network to a degree 6 Legendre interpolant
of a rational function.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;rational_poly_poly_init_net.png&#34; alt=&#34;Polynomial Initialized network&#34; width=&#34;400&#34;/&gt;&lt;/p&gt;

&lt;p&gt;This network can then be trained to achieve a good approximation of the target
rational polynomial.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;rational_poly_trained_net.png&#34; alt=&#34;Trained Network&#34; width=&#34;400&#34;/&gt;&lt;/p&gt;

&lt;p&gt;There are some recovery artifacts around the edges of the rational function.
We believe these are due to the fact that we did not allow
connections to form between the interior nodes of subnetworks which
approximate $x^2$. We can alieve these effects by using a simple
polynomial $x^2$ to initialize a network.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;cos_1d_deep_net_both_trained_net_3.png&#34; alt=&#34;Realistic architecture network&#34; width=&#34;400&#34;/&gt;&lt;/p&gt;

&lt;p&gt;Above we compare the result of trianing the same network initialized by our method to
the popular Xavier random initiailzation. We see that our network was better able to
approximate the target, at least for this example.&lt;/p&gt;

&lt;p&gt;In the future, we plan to explore using this initialization scheme in real-world
problems of interest such as classificaiton.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Deep Neural Network Architecture Inspired by Polynomial Approximation</title>
      <link>https://joedaws.github.io/talk/at16-vanderbilt/</link>
      <pubDate>Mon, 13 May 2019 16:40:32 -0400</pubDate>
      
      <guid>https://joedaws.github.io/talk/at16-vanderbilt/</guid>
      <description></description>
    </item>
    
    <item>
      <title>SIAM CSE 2019 Rom Minisymposium</title>
      <link>https://joedaws.github.io/talk/siam-cse-2019-rom-minisymposium/</link>
      <pubDate>Wed, 20 Feb 2019 08:58:37 -0500</pubDate>
      
      <guid>https://joedaws.github.io/talk/siam-cse-2019-rom-minisymposium/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Spectral Clustering SNS Target Data</title>
      <link>https://joedaws.github.io/project/spectral-clustering-sns-target-data/</link>
      <pubDate>Thu, 08 Nov 2018 11:09:54 -0500</pubDate>
      
      <guid>https://joedaws.github.io/project/spectral-clustering-sns-target-data/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Weighted L1 Minimization and Closed Trees</title>
      <link>https://joedaws.github.io/publication/weighted-l1-minimization-and-closed-trees/</link>
      <pubDate>Thu, 08 Nov 2018 10:13:35 -0500</pubDate>
      
      <guid>https://joedaws.github.io/publication/weighted-l1-minimization-and-closed-trees/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Designing Neural Networks</title>
      <link>https://joedaws.github.io/post/designing-neural-networks/</link>
      <pubDate>Wed, 07 Nov 2018 15:23:38 -0500</pubDate>
      
      <guid>https://joedaws.github.io/post/designing-neural-networks/</guid>
      <description>

&lt;h2 id=&#34;towards-a-mathematical-understanding-of-using-neural-networks-for-function-approximation&#34;&gt;Towards a mathematical understanding of using neural networks for function approximation.&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;An artificial neural network (ANN) is a computational framework
inspired by the interactions of neurons in the brain.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Recently many difficult problems have been effectively solved using
algorithms and mathematical models which iteratively improve their
performance based on data rather than explicitly finding a solution.
Such methods, which primarily rely on data rather than explicit solution
techniques, can be grouped together under the umbrella of &lt;strong&gt;Machine Learning&lt;/strong&gt;.
The field of machine learning is rapidly exanding and new results often
draw on a diverse background of mathematical, statistical, and even physics based
methods. However, many of its most dramatic successes involve neural networks.
Some examples I find interesting are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cs.toronto.edu/~fritz/absps/ncfast.pdf&#34; target=&#34;_blank&#34;&gt;Image Classification&lt;/a&gt;
&amp;ndash; A neural network is trained to classify handwritten numbers using a
dataset of pre-labelled images of handwritten numbers. It is even able
to classify images of digits not contained in the training set.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.nature.com/articles/nature24270&#34; target=&#34;_blank&#34;&gt;Artificial intelligence&lt;/a&gt;
&amp;ndash; Move prediction and selection for the game &amp;ldquo;Go&amp;rdquo; by a neural network.
This network was able to beat a top level human player.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.openai.com/learning-dexterity/&#34; target=&#34;_blank&#34;&gt;Reinforced Learning&lt;/a&gt;
&amp;ndash; A robotic hand controlled by a neural network is trained to manipulate
a cube to requested positions. The network uses data from a camera to
assess the current state of the cube and choose actions for the hand to
perform in order to rotate it to a desired position.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All three of these examples employ a neural network to solve a complex problem.&lt;/p&gt;

&lt;p&gt;There are many fundamental theoretical questions that remain
(partially) unanswered about ANNs, e.g.,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;All networks require some training, a process by which pairs of inputs and outputs
are used to set the internal parameters of the model. A popular strategy for training
is accomplished by stochastic gradient descent
&lt;a href=&#34;https://en.wikipedia.org/wiki/Stochastic_gradient_descent&#34; target=&#34;_blank&#34;&gt;(SDG)&lt;/a&gt;.
The convergence of this optimization problem is not well
quantified or well understood.&lt;/li&gt;
&lt;li&gt;Given a network with a particular architecture, how does one determine
the class of functions it can express?
Moreover, even if optimal parameters can be shown to exist, are they easy to learn?&lt;/li&gt;
&lt;li&gt;Under what conditions will a network generalize well to inputs and
outputs not contained in the training set? Some networks experience
very bad overfitting and do not make good predictions for new incoming data.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In order to make sense of any of these theoretical considerations
in a mathematical context it is useful to characterize ANN&amp;rsquo;s as both
functions and graphs.
In this post we will consider designing a network to approximate a
given analytic function $f$.
As a function, the network can be interpreted as a mapping from $\mathbb{R}^d$ to
$\mathbb{R}^k$ where $d$ is the number of inputs to the network and
$k$ is the number of ouputs. Let
$$u_{NN}:\mathbb{R}^d \rightarrow \mathbb{R}^k$$
be the functional interpretation of the neural network approximating an
analytic function $u$. The function $u_{NN}$ is computed by a
combining a series computations performed
by constituent functions. The interactions between the constituent
functions that make up the network can be described by a
&lt;a href=&#34;https://en.wikipedia.org/wiki/Directed_acyclic_graph&#34; target=&#34;_blank&#34;&gt;directed acylic graph&lt;/a&gt;.
The associated graph can be used to analyze the complexity of the
network defined as the number of trainable parameters.&lt;/p&gt;

&lt;p&gt;Simply put, a neural network can be characterized as a function consisting of
compositions and linear combinations of constituent functions called
&lt;strong&gt;neurons&lt;/strong&gt; which perform a computation of the form
$$ \sigma \left( \sum_{i=1}^k w_i \cdot x_i  + b \right).$$&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$x_i$ &amp;ndash; The &lt;strong&gt;inputs&lt;/strong&gt; for the computation.
They could be the result of computations performed by other neurons or
raw input.&lt;/li&gt;
&lt;li&gt;$w_i$ &amp;ndash; The &lt;strong&gt;weight&lt;/strong&gt; associated with the incoming data $x_i$.&lt;/li&gt;
&lt;li&gt;$\sigma$ &amp;ndash; A nonlinear function called the &lt;strong&gt;activation function&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;$b$ &amp;ndash; A number called the &lt;strong&gt;bias&lt;/strong&gt; of the neuron.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;nonlinear-functions-and-graphs&#34;&gt;Nonlinear functions and graphs&lt;/h2&gt;

&lt;p&gt;Below is a simple example showing the dual characterization
of a single hidden layer ANN.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;featured.png&#34; alt=&#34;single layer network&#34; /&gt;
&lt;em&gt;A single hidden layer network with three neurons.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This graph depicts how each of the constituent computations are used
to form the final output of the network. Here we call the function
induced by the network $f$. The function $f$ maps the input $x$
to the output $f(x)$ and is computed as a linear combination of
the computations performed in the hidden layer.
One can also write $f$ so that it is easily recognized as a funciton.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;network_function.png&#34; alt=&#34;functional network&#34; /&gt;
&lt;em&gt;The network as a function.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This representation makes explicit the mapping how $f$ maps the input
$x$ to the output $f(x)$. It also shows how the mapping induced
by the network depends entirely on the choice of the weights, biases and
the activation function. Choosing these parameters directly determines
how well the network $f$ performs its task.&lt;/p&gt;

&lt;p&gt;Suppose that we wish to design a network for function approximation,
i.e., one is tasked to choose the parameters of $f$ so that it
produces the same input&amp;ndash;output pairs as a function
$g:\mathbb{R} \rightarrow \mathbb{R}$.
The problem of finding an optimal approximation of $g$ is then finding
the optimal parameters for $f$ so that the quantity $error$ is as small
as possible where
$$ error = ||f - g||$$
for an appropriately chosen norm $|| \cdot ||$.
Notice that the approximation power of $f$ also depends on the choice of
$\sigma$. It is possible to consider $\sigma$ as a parameter, but there are
several established choices for $\sigma$ that are widely used, see e.g.
&lt;a href=&#34;https://en.wikipedia.org/wiki/Activation_function&#34; target=&#34;_blank&#34;&gt;activation functions&lt;/a&gt;.
Here I will consider using the so called Rectified Linear Unit (ReLU)
activation function.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;relu.png&#34; alt=&#34;relu&#34; /&gt;
&lt;em&gt;The ReLU actication function is a piecewise linear function.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The network $f$ is a very simple network. Networks with a single hidden
layer are called shallow networks and in practice shallow networks
have been shown to have limited usage. In fact, it is known that deep networks, i.e.
networks with more than one hidden layer, are more expressive than
shallow in the sense that they require less complex structure in order to
express a given funciton. See the work of
&lt;a href=&#34;https://pdfs.semanticscholar.org/694a
/d455c119c0d07036792b80abbf5488a9a4ca.pdf&#34; target=&#34;_blank&#34;&gt;Mhaskar&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;the-architecture-of-a-network-depends-on-its-application&#34;&gt;The architecture of a network depends on its application&lt;/h2&gt;

&lt;p&gt;In practice, the design of neural networks is linked to their
desired application. For instance, neural networks used for image
classification typically involve
&lt;a href=&#34;https://en.wikipedia.org/wiki/Convolutional_neural_network&#34; target=&#34;_blank&#34;&gt;convolutional kernels&lt;/a&gt;
which extract local features. To my knowledge there is no clear
theoretical justification of this choice of architecture. However,
since there are successful image processing methods which
exploit local and global image information simultaneously in a similar
way to convolutional layers in a neural network, .e.g.,
&lt;a href=&#34;ftp://ftp.math.ucla.edu/pub/camreport/cam16-04.pdf&#34; target=&#34;_blank&#34;&gt;LDMM&lt;/a&gt; and
&lt;a href=&#34;https://ieeexplore.ieee.org/document/1467423&#34; target=&#34;_blank&#34;&gt;NONLOCAL MEANS&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;When designing a network to approximate a function it is therefore
reasonable for the architecture of a proposed network to
depend on a particular function approximation scheme or at least
be inspired by an approximation scheme.
This choice also allows one to take advantage of the large body
of theoretical work describing methods to
approximate functions (of various levels of smoothness and dimension).
I&amp;rsquo;m currently analyzing a network based on polynomial approximation of
high-dimensional functions as well as implementing one.&lt;/p&gt;

&lt;p&gt;Check this post in the near future for
some preliminary numerical results of this network as well as some
theoretical estimates of its expressive power.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>IMI Graduate Student Mini Conference</title>
      <link>https://joedaws.github.io/talk/imi-graduate-student-mini-conf/</link>
      <pubDate>Wed, 07 Nov 2018 13:02:28 -0500</pubDate>
      
      <guid>https://joedaws.github.io/talk/imi-graduate-student-mini-conf/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Image Processing and Optimization</title>
      <link>https://joedaws.github.io/project/image-inpainting/</link>
      <pubDate>Mon, 05 Nov 2018 09:48:32 -0500</pubDate>
      
      <guid>https://joedaws.github.io/project/image-inpainting/</guid>
      <description>

&lt;h1 id=&#34;weighted-ell-1-minimization&#34;&gt;Weighted $\ell_1$-minimization&lt;/h1&gt;

&lt;p&gt;In an upcoming work a convex optimization approach,
based on weighted $\ell_1$-regularization, is proposed
for reconstructing sparse wavelet representations.
We show our proposed optimization problem
is effective for solving the signal/image inpainting and denoising problems.
We take the funcational representation of an image or signal to be
$$ f(y) = \sum_{j \in \mathcal{J}} c_j \Psi_j(y) . $$
The wavelet coefficients $c = (c_j)_{j \in \mathcal{J}}$
associated to the functional representation of the
object of interest are obtained as minizers of&lt;br /&gt;
$$ \min_{c \in \mathbb{R}^N} \lVert c \rVert_{\omega,1} + \lVert Ac - f \rVert_2, $$
where $f \in \mathbb{R}$ is either a vector $m$ subsamples or $m$
noisy observations.&lt;br /&gt;
We show that by choosing the weights to be the uniform norms of the wavelet functions,
i.e., the $L^\infty$-norm of $\Psi_j$,
the support of the recovered vector of coefficients forms a particular kind of index set,
a closed tree.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;closed_tree.png&#34; alt=&#34;Closed Tree&#34; /&gt;
&lt;em&gt;An example of a closed tree. The solid edges form the closed tree.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This kind of index set is consistent with the behavior of many real-world signals and
images and therefore our approach applies to a wide class of signals.
Furthermore, the numerical examples below show the effectiveness
of weighted $\ell_1$-minimization. In addition, we have shown
that the sample complexity associated with the weighted
$\ell_1$-regularized problem is smaller than the sample complexity
of the unweighted problem. This analysis will appear in an upcoming paper.&lt;/p&gt;

&lt;h2 id=&#34;image-inpainting&#34;&gt;Image Inpainting&lt;/h2&gt;

&lt;p&gt;Compare the recovery of a picture of flamingos using unweighted and weighted
$\ell_1$-minimization from a random sample of $8\%$ of the pixels.
The original image is a $972 \times 1296$ pixel image. We used the
&lt;a href=&#34;https://wikipedia.org/wiki/daubechies_wavelet&#34; target=&#34;_blank&#34;&gt;Daubechies 3&lt;/a&gt; &lt;em&gt;db3&lt;/em&gt; basis.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;fig_flamingos_orig.png&#34; alt=&#34;original flamingos&#34; /&gt;
&lt;img src=&#34;fig_flamingos_unweighted_8p.png&#34; alt=&#34;unweighted flamingos&#34; /&gt;
&lt;img src=&#34;fig_flamingos_weighted_8p.png&#34; alt=&#34;weighted flamingos&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;image-de-noising&#34;&gt;Image De-noising&lt;/h2&gt;

&lt;p&gt;The weighted $\ell_1$-minimization problem can also be used to de-noise an image:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;fig_lighthouse_noisy.png&#34; alt=&#34;original lighthouse&#34; /&gt;
&lt;img src=&#34;fig_lighthouse_unweighted_denoised.png&#34; alt=&#34;unweighted lighthouse&#34; /&gt;
&lt;img src=&#34;fig_lighthouse_weighted_denoised.png&#34; alt=&#34;weighted lighthouse&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
