<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Joseph Daws on Joseph Daws</title>
    <link>https://joedaws.github.io/</link>
    <description>Recent content in Joseph Daws on Joseph Daws</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 -0400</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>SIAM CSE 2019 Rom Minisymposium</title>
      <link>https://joedaws.github.io/talk/siam-cse-2019-rom-minisymposium/</link>
      <pubDate>Wed, 20 Feb 2019 08:58:37 -0500</pubDate>
      
      <guid>https://joedaws.github.io/talk/siam-cse-2019-rom-minisymposium/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Networks with Binary Tree Architecture</title>
      <link>https://joedaws.github.io/post/binary-tree-network/</link>
      <pubDate>Mon, 14 Jan 2019 11:36:57 -0500</pubDate>
      
      <guid>https://joedaws.github.io/post/binary-tree-network/</guid>
      <description>

&lt;h2 id=&#34;binary-trees&#34;&gt;Binary Trees&lt;/h2&gt;

&lt;p&gt;Binary trees are both interesting from a purely graph theorectic point of view
as well as in application.
Their usefulness is evident by their wide use as a &lt;a href=&#34;https://en.wikipedia.org/wiki/Tree_structure&#34; target=&#34;_blank&#34;&gt;model&lt;/a&gt;
for organization in a wide variety of applications.
For example, they are used to describe directory
hierachies in file systems, to store data so that it may be effeciently
queried and to describe and organize the possible game states
such as all possible chess matches.&lt;/p&gt;

&lt;p&gt;In this article we will discuss a class of functions which
is composed of compositions of simpler functions whose
interaction is described by a binary tree. Consider
the following diagram of a binary tree.&lt;/p&gt;

&lt;p&gt;INSERT DIAGRAM&lt;/p&gt;

&lt;p&gt;This graph corresponds to the function
$H:\mathcal{R}^d \rightarrow \mathcal{R}$ given by&lt;/p&gt;

&lt;p&gt;$$
H(x_1,\dots,x_d) := h_{\nu_0}(h_{\nu_{1}}(x_1,x_2),
                                  h_{\nu_{2}}(x_3,x_4))
$$&lt;/p&gt;

&lt;p&gt;some functions are compositions of other functions
have a so-called binary tree structure.&lt;/p&gt;

&lt;h2 id=&#34;polynoials-as-products&#34;&gt;Polynoials as Products&lt;/h2&gt;

&lt;p&gt;An example of a polynomial with locally hierarchical compositional structure
was considered by &lt;a href=&#34;https://arxiv.org/abs/1611.00740&#34; target=&#34;_blank&#34;&gt;Poggio et al.&lt;/a&gt;.
They consider the polynomial
$$ Q(x_1,x_2,\dots,x_N) $$&lt;/p&gt;

&lt;p&gt;Polynomials can be written as the product of
complex numbers of the form $(x_i - r_j)$.&lt;/p&gt;

&lt;h2 id=&#34;locally-hierarchical-basis-functions&#34;&gt;Locally Hierarchical Basis Functions&lt;/h2&gt;

&lt;p&gt;Review the theory of Poggio et al for describing the
expressibility of binvary tree neural networks.&lt;/p&gt;

&lt;p&gt;Of course, all of this can be generalized to a $k-ary$ tree, i.e.,
a tree where each node has at most $k$ children.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Spectral Clustering SNS Target Data</title>
      <link>https://joedaws.github.io/project/spectral-clustering-sns-target-data/</link>
      <pubDate>Thu, 08 Nov 2018 11:09:54 -0500</pubDate>
      
      <guid>https://joedaws.github.io/project/spectral-clustering-sns-target-data/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Weighted L1 Minimization and Closed Trees</title>
      <link>https://joedaws.github.io/publication/weighted-l1-minimization-and-closed-trees/</link>
      <pubDate>Thu, 08 Nov 2018 10:13:35 -0500</pubDate>
      
      <guid>https://joedaws.github.io/publication/weighted-l1-minimization-and-closed-trees/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Designing Neural Networks</title>
      <link>https://joedaws.github.io/post/designing-neural-networks/</link>
      <pubDate>Wed, 07 Nov 2018 15:23:38 -0500</pubDate>
      
      <guid>https://joedaws.github.io/post/designing-neural-networks/</guid>
      <description>

&lt;h2 id=&#34;towards-a-mathematical-understanding-of-using-neural-networks-for-function-approximation&#34;&gt;Towards a mathematical understanding of using neural networks for function approximation.&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;An artificial neural network (ANN) is a computational framework
inspired by the interactions of neurons in the brain.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Recently many difficult problems have been effectively solved using
algorithms and mathematical models which iteratively improve their
performance based on data rather than explicitly finding a solution.
Such methods, which primarily rely on data rather than explicit solution
techniques, can be grouped together under the umbrella of &lt;strong&gt;Machine Learning&lt;/strong&gt;.
The field of machine learning is rapidly exanding and new results often
draw on a diverse background of mathematical, statistical, and even physics based
methods. However, many of its most dramatic successes involve neural networks.
Some examples I find interesting are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cs.toronto.edu/~fritz/absps/ncfast.pdf&#34; target=&#34;_blank&#34;&gt;Image Classification&lt;/a&gt;
&amp;ndash; A neural network is trained to classify handwritten numbers using a
dataset of pre-labelled images of handwritten numbers. It is even able
to classify images of digits not contained in the training set.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.nature.com/articles/nature24270&#34; target=&#34;_blank&#34;&gt;Artificial intelligence&lt;/a&gt;
&amp;ndash; Move prediction and selection for the game &amp;ldquo;Go&amp;rdquo; by a neural network.
This network was able to beat a top level human player.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.openai.com/learning-dexterity/&#34; target=&#34;_blank&#34;&gt;Reinforced Learning&lt;/a&gt;
&amp;ndash; A robotic hand controlled by a neural network is trained to manipulate
a cube to requested positions. The network uses data from a camera to
assess the current state of the cube and choose actions for the hand to
perform in order to rotate it to a desired position.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All three of these examples employ a neural network to solve a complex problem.&lt;/p&gt;

&lt;p&gt;There are many fundamental theoretical questions that remain
(partially) unanswered about ANNs, e.g.,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;All networks require some training, a process by which pairs of inputs and outputs
are used to set the internal parameters of the model. A popular strategy for training
is accomplished by stochastic gradient descent
&lt;a href=&#34;https://en.wikipedia.org/wiki/Stochastic_gradient_descent&#34; target=&#34;_blank&#34;&gt;(SDG)&lt;/a&gt;.
The convergence of this optimization problem is not well
quantified or well understood.&lt;/li&gt;
&lt;li&gt;Given a network with a particular architecture, how does one determine
the class of functions it can express?
Moreover, even if optimal parameters can be shown to exist, are they easy to learn?&lt;/li&gt;
&lt;li&gt;Under what conditions will a network generalize well to inputs and
outputs not contained in the training set? Some networks experience
very bad overfitting and do not make good predictions for new incoming data.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In order to make sense of any of these theoretical considerations
in a mathematical context it is useful to characterize ANN&amp;rsquo;s as both
functions and graphs.
In this post we will consider designing a network to approximate a
given analytic function $f$.
As a function, the network can be interpreted as a mapping from $\mathbb{R}^d$ to
$\mathbb{R}^k$ where $d$ is the number of inputs to the network and
$k$ is the number of ouputs. Let
$$u_{NN}:\mathbb{R}^d \rightarrow \mathbb{R}^k$$
be the functional interpretation of the neural network approximating an
analytic function $u$. The function $u_{NN}$ is computed by a
combining a series computations performed
by constituent functions. The interactions between the constituent
functions that make up the network can be described by a
&lt;a href=&#34;https://en.wikipedia.org/wiki/Directed_acyclic_graph&#34; target=&#34;_blank&#34;&gt;directed acylic graph&lt;/a&gt;.
The associated graph can be used to analyze the complexity of the
network defined as the number of trainable parameters.&lt;/p&gt;

&lt;p&gt;Simply put, a neural network can be characterized as a function consisting of
compositions and linear combinations of constituent functions called
&lt;strong&gt;neurons&lt;/strong&gt; which perform a computation of the form
$$ \sigma \left( \sum_{i=1}^k w_i \cdot x_i  + b \right).$$&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$x_i$ &amp;ndash; The &lt;strong&gt;inputs&lt;/strong&gt; for the computation.
They could be the result of computations performed by other neurons or
raw input.&lt;/li&gt;
&lt;li&gt;$w_i$ &amp;ndash; The &lt;strong&gt;weight&lt;/strong&gt; associated with the incoming data $x_i$.&lt;/li&gt;
&lt;li&gt;$\sigma$ &amp;ndash; A nonlinear function called the &lt;strong&gt;activation function&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;$b$ &amp;ndash; A number called the &lt;strong&gt;bias&lt;/strong&gt; of the neuron.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;nonlinear-functions-and-graphs&#34;&gt;Nonlinear functions and graphs&lt;/h2&gt;

&lt;p&gt;Below is a simple example showing the dual characterization
of a single hidden layer ANN.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;featured.png&#34; alt=&#34;single layer network&#34; /&gt;
&lt;em&gt;A single hidden layer network with three neurons.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This graph depicts how each of the constituent computations are used
to form the final output of the network. Here we call the function
induced by the network $f$. The function $f$ maps the input $x$
to the output $f(x)$ and is computed as a linear combination of
the computations performed in the hidden layer.
One can also write $f$ so that it is easily recognized as a funciton.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;network_function.png&#34; alt=&#34;functional network&#34; /&gt;
&lt;em&gt;The network as a function.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This representation makes explicit the mapping how $f$ maps the input
$x$ to the output $f(x)$. It also shows how the mapping induced
by the network depends entirely on the choice of the weights, biases and
the activation function. Choosing these parameters directly determines
how well the network $f$ performs its task.&lt;/p&gt;

&lt;p&gt;Suppose that we wish to design a network for function approximation,
i.e., one is tasked to choose the parameters of $f$ so that it
produces the same input&amp;ndash;output pairs as a function
$g:\mathbb{R} \rightarrow \mathbb{R}$.
The problem of finding an optimal approximation of $g$ is then finding
the optimal parameters for $f$ so that the quantity $error$ is as small
as possible where
$$ error = ||f - g||$$
for an appropriately chosen norm $|| \cdot ||$.
Notice that the approximation power of $f$ also depends on the choice of
$\sigma$. It is possible to consider $\sigma$ as a parameter, but there are
several established choices for $\sigma$ that are widely used, see e.g.
&lt;a href=&#34;https://en.wikipedia.org/wiki/Activation_function&#34; target=&#34;_blank&#34;&gt;activation functions&lt;/a&gt;.
Here I will consider using the so called Rectified Linear Unit (ReLU)
activation function.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;relu.png&#34; alt=&#34;relu&#34; /&gt;
&lt;em&gt;The ReLU actication function is a piecewise linear function.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The network $f$ is a very simple network. Networks with a single hidden
layer are called shallow networks and in practice shallow networks
have been shown to have limited usage. In fact, it is known that deep networks, i.e.
networks with more than one hidden layer, are more expressive than
shallow in the sense that they require less complex structure in order to
express a given funciton. See the work of
&lt;a href=&#34;https://pdfs.semanticscholar.org/694a
/d455c119c0d07036792b80abbf5488a9a4ca.pdf&#34; target=&#34;_blank&#34;&gt;Mhaskar&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;the-architecture-of-a-network-depends-on-its-application&#34;&gt;The architecture of a network depends on its application&lt;/h2&gt;

&lt;p&gt;In practice, the design of neural networks is linked to their
desired application. For instance, neural networks used for image
classification typically involve
&lt;a href=&#34;https://en.wikipedia.org/wiki/Convolutional_neural_network&#34; target=&#34;_blank&#34;&gt;convolutional kernels&lt;/a&gt;
which extract local features. To my knowledge there is no clear
theoretical justification of this choice of architecture. However,
since there are successful image processing methods which
exploit local and global image information simultaneously in a similar
way to convolutional layers in a neural network, .e.g.,
&lt;a href=&#34;ftp://ftp.math.ucla.edu/pub/camreport/cam16-04.pdf&#34; target=&#34;_blank&#34;&gt;LDMM&lt;/a&gt; and
&lt;a href=&#34;https://ieeexplore.ieee.org/document/1467423&#34; target=&#34;_blank&#34;&gt;NONLOCAL MEANS&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;When designing a network to approximate a function it is therefore
reasonable for the architecture of a proposed network to
depend on a particular function approximation scheme or at least
be inspired by an approximation scheme.
This choice also allows one to take advantage of the large body
of theoretical work describing methods to
approximate functions (of various levels of smoothness and dimension).
I&amp;rsquo;m currently analyzing a network based on polynomial approximation of
high-dimensional functions as well as implementing one.&lt;/p&gt;

&lt;p&gt;Check this post in the near future for
some preliminary numerical results of this network as well as some
theoretical estimates of its expressive power.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>IMI Graduate Student Mini Conference</title>
      <link>https://joedaws.github.io/talk/imi-graduate-student-mini-conf/</link>
      <pubDate>Wed, 07 Nov 2018 13:02:28 -0500</pubDate>
      
      <guid>https://joedaws.github.io/talk/imi-graduate-student-mini-conf/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Image Processing and Optimization</title>
      <link>https://joedaws.github.io/project/image-inpainting/</link>
      <pubDate>Mon, 05 Nov 2018 09:48:32 -0500</pubDate>
      
      <guid>https://joedaws.github.io/project/image-inpainting/</guid>
      <description>

&lt;h1 id=&#34;weighted-ell-1-minimization&#34;&gt;Weighted $\ell_1$-minimization&lt;/h1&gt;

&lt;p&gt;In an upcoming work a convex optimization approach,
based on weighted $\ell_1$-regularization, is proposed
for reconstructing sparse wavelet representations.
We show our proposed optimization problem
is effective for solving the signal/image inpainting and denoising problems.
We take the funcational representation of an image or signal to be
$$ f(y) = \sum_{j \in \mathcal{J}} c_j \Psi_j(y) . $$
The wavelet coefficients $c = (c_j)_{j \in \mathcal{J}}$
associated to the functional representation of the
object of interest are obtained as minizers of&lt;br /&gt;
$$ \min_{c \in \mathbb{R}^N} \lVert c \rVert_{\omega,1} + \lVert Ac - f \rVert_2, $$
where $f \in \mathbb{R}$ is either a vector $m$ subsamples or $m$
noisy observations.&lt;br /&gt;
We show that by choosing the weights to be the uniform norms of the wavelet functions,
i.e., the $L^\infty$-norm of $\Psi_j$,
the support of the recovered vector of coefficients forms a particular kind of index set,
a closed tree.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;closed_tree.png&#34; alt=&#34;Closed Tree&#34; /&gt;
&lt;em&gt;An example of a closed tree. The solid edges form the closed tree.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This kind of index set is consistent with the behavior of many real-world signals and
images and therefore our approach applies to a wide class of signals.
Furthermore, the numerical examples below show the effectiveness
of weighted $\ell_1$-minimization. In addition, we have shown
that the sample complexity associated with the weighted
$\ell_1$-regularized problem is smaller than the sample complexity
of the unweighted problem. This analysis will appear in an upcoming paper.&lt;/p&gt;

&lt;h2 id=&#34;image-inpainting&#34;&gt;Image Inpainting&lt;/h2&gt;

&lt;p&gt;Compare the recovery of a picture of flamingos using unweighted and weighted
$\ell_1$-minimization from a random sample of $8\%$ of the pixels.
The original image is a $972 \times 1296$ pixel image. We used the
&lt;a href=&#34;https://wikipedia.org/wiki/daubechies_wavelet&#34; target=&#34;_blank&#34;&gt;Daubechies 3&lt;/a&gt; &lt;em&gt;db3&lt;/em&gt; basis.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;fig_flamingos_orig.png&#34; alt=&#34;original flamingos&#34; /&gt;
&lt;img src=&#34;fig_flamingos_unweighted_8p.png&#34; alt=&#34;unweighted flamingos&#34; /&gt;
&lt;img src=&#34;fig_flamingos_weighted_8p.png&#34; alt=&#34;weighted flamingos&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;image-de-noising&#34;&gt;Image De-noising&lt;/h2&gt;

&lt;p&gt;The weighted $\ell_1$-minimization problem can also be used to de-noise an image:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;fig_lighthouse_noisy.png&#34; alt=&#34;original lighthouse&#34; /&gt;
&lt;img src=&#34;fig_lighthouse_unweighted_denoised.png&#34; alt=&#34;unweighted lighthouse&#34; /&gt;
&lt;img src=&#34;fig_lighthouse_weighted_denoised.png&#34; alt=&#34;weighted lighthouse&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
