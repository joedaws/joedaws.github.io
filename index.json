[
  {
    "authors": [
      "Joseph Daws Jr.",
      "Clayton Webster"
    ],
    "categories": null,
    "content": "",
    "date": 1559077852,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1559077852,
    "objectID": "b16bb80e84be7a1e76e7e385f7424d6d",
    "permalink": "https://joedaws.github.io/publication/polynomials-for-nets/",
    "publishdate": "2019-05-28T17:10:52-04:00",
    "relpermalink": "/publication/polynomials-for-nets/",
    "section": "publication",
    "summary": "In this effort we propose a novel approach for reconstructing multivariate functions from training data, by identifying both a suitable network architecture and an initialization using polynomial-based approximations. Training deep neural networks using gradient descent can be interpreted as moving the set of network parameters along the loss landscape in order to minimize the loss functional. The initialization of parameters is important for iterative training methods based on descent. Our procedure produces a network whose initial state is a polynomial representation of the training data. The major advantage of this technique is from this initialized state the network may be improved using standard training procedures. Since the network already approximates the data, training is more likely to produce a set of parameters associated with a desirable local minimum. We provide the details of the theory necessary for constructing such networks and also consider several numerical examples that reveal our approach ultimately produces networks which can be effectively trained from our initialized state to achieve an improved approximation for a large class of target functions.",
    "tags": [
      "deep-learning",
      "neural-networks",
      "approximation-theory"
    ],
    "title": "A Polynomial-Based Approach for Architectural Design and Learning with Deep Neural Networks",
    "type": "publication"
  },
  {
    "authors": [
      "Joseph Daws"
    ],
    "categories": [
      "Neural-Networks"
    ],
    "content": " Polynomial approximation can inform network design The following is a breif discussion of work found in this preprint which has been submitted to NeurIPS 2019. In this post I\u0026rsquo;ve left out many details. If you are interested in these details please see the preprint. As described in my previous post Neural networks have emerged as a very powerful tool for constructing nonlinear functions in high-dimensions. They are known to be very expressive in the sense that the class of functions they can approximate is very wide. However, in order to cosntruct an approximation of a given function a very large number of network parameters must be determined. This is espcially true for Deep Neural Networks (DNNs). Because DNNs have many parameters and because the functions assocaited with solving tasks like image classification many be quite complicated, training can be very difficult. Although many aspects of neural network are new (such as fast training algorithms implementable on a GPU) their expressive power and ability to appoximate has been known for decades. However, there do not seem to be many works which make explicit connections between the powerful results of classical approximation theory to expressibility results for neural networks. Below is an outline of one way in which polynomial approximation may inform efficient approximation of complicated functions by networks. The primary connection between these kinds of approximation is made by by constructing a network which can approximate a given polynomial. One can then take advantage of existing polynomial approximate results by first finding a suitable polynomial approximation then constructing a network which achieves this approximation. After it has been initialzed to behave like a polynomial, the network can be trained. Our numerical examples show that networks initialized by our proposed method have better performance than the polynomial used to initialize them.\nA Network for Approximating Polynomials Polynomials are natural objects of interest in many different areas of mathematics. A generic degree $n$ polynomial $P_n$ is often written $$ P_n(x) = a_0 + a_1 x + a_2 x^2 + \\cdots + a_n x^n. $$ Written in this way the polynomial $P_n$ is completely characterized by its coefficients $a_i$ for $i=0,\u0026hellip;,n$. Therefore, finding a suitable polynomial approximation is equivalent to identifying coefficients. Instead of writing the polynomial as a sum of the powers of the input $x$ one may also write a polynomial as the product of linear shifts of the input. According to the Fundamental Theorem of Algebra every degree $n$ polynomial can be written $$P_n(x) = a\\Pi_{i=1}^n (x - r_i)$$ for $n$ complex numbers $r_i$ and where $a$ is a scaling factor. We construct a network which approximates $d$-dimensional generalizations of polynomials like $P$ by approximating the product of $n$ numbers with a network. Such a network can be constructed by simplying chaining together several instances of a network which approximates the product of two numbers. Noticing that for two real numbers $a$ and $b$ $$ 2ab = (a+b)^2 -a^2 - b^2$$, we can approximate a product by a linear combination of the outputs of three networks. One which approximtes $(a+b)^2$, one which approximates $a^2$ and one which approximates $b^2$. A network which approximtaes the mapping $x \\mapsto x^2$ can be constructed from an $L$ layer network with $4$ nodes per level.\nChaining several of these networks together we can approximate the product of $n$ numbers and hence a polynomial. The following is a rough outline the structure of network which approximates a polynomial.\nInitialization based on polynomials It is possible to initialize a network to have polynomial behavior. Below, we have initiailzed network to a degree 6 Legendre interpolant of a rational function.\nThis network can then be trained to achieve a good approximation of the target rational polynomial.\nThere are some recovery artifacts around the edges of the rational function. We believe these are due to the fact that we did not allow connections to form between the interior nodes of subnetworks which approximate $x^2$. We can alieve these effects by using a simple polynomial $x^2$ to initialize a network.\nAbove we compare the result of trianing the same network initialized by our method to the popular Xavier random initiailzation. We see that our network was better able to approximate the target, at least for this example.\nIn the future, we plan to explore using this initialization scheme in real-world problems of interest such as classificaiton.\n",
    "date": 1557851817,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1557851817,
    "objectID": "64067b5be8b009f1bee67e2454493c28",
    "permalink": "https://joedaws.github.io/post/binary-tree-network/",
    "publishdate": "2019-05-14T11:36:57-05:00",
    "relpermalink": "/post/binary-tree-network/",
    "section": "post",
    "summary": "Polynomial approximation can inform network design The following is a breif discussion of work found in this preprint which has been submitted to NeurIPS 2019. In this post I\u0026rsquo;ve left out many details. If you are interested in these details please see the preprint. As described in my previous post Neural networks have emerged as a very powerful tool for constructing nonlinear functions in high-dimensions. They are known to be very expressive in the sense that the class of functions they can approximate is very wide.",
    "tags": [
      "neural-networks",
      "network-architecture",
      "machine-learning"
    ],
    "title": "Polynomial Inspired Neural Network Design and Initiailzation",
    "type": "post"
  },
  {
    "authors": [
      "Joseph Daws"
    ],
    "categories": null,
    "content": "",
    "date": 1557780032,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1557780032,
    "objectID": "e82f8b5681b3d6780a3fdd2a073da64f",
    "permalink": "https://joedaws.github.io/talk/at16-vanderbilt/",
    "publishdate": "2019-05-13T16:40:32-04:00",
    "relpermalink": "/talk/at16-vanderbilt/",
    "section": "talk",
    "summary": "We introduce a class of deep neural networks whose architecture is inspired by polynomial approximation and which can be used for function approximation. Deep neural networks have been deployed for solving many challenging tasks. Despite the immense amount of recent research on neural networks choosing a network architecture and initialization for a given problem is difficult. We show that the parameters of our proposed network can be initialized so that it approximates a given polynomial function. If the chosen polynomial is itself an approximation of a function of interest, our network performs at least as well as the polynomial approximation. We then consider training our constructed network and show several examples where the network achieves an approximation better than the polynomial approximation used to initialize its parameters.",
    "tags": [
      "neural-networks"
    ],
    "title": "A Deep Neural Network Architecture Inspired by Polynomial Approximation",
    "type": "talk"
  },
  {
    "authors": [
      "Joseph Daws",
      "Armenak Petrosyan",
      "Hoang Tran",
      "Clayton Webster"
    ],
    "categories": null,
    "content": "",
    "date": 1550671117,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1550671117,
    "objectID": "748678c9397a1f785af3c14a84eee0d6",
    "permalink": "https://joedaws.github.io/talk/siam-cse-2019-rom-minisymposium/",
    "publishdate": "2019-02-20T08:58:37-05:00",
    "relpermalink": "/talk/siam-cse-2019-rom-minisymposium/",
    "section": "talk",
    "summary": "Real-world images and signals are known to be sparse in a wavelet basis. In this work, we propose a convex optimization approach, based on weighted l1-regularization, for reconstructing a signal of interest by identifying the coefficients in a sparse wavelet approximation. The vector of wavelet coefficients that form this approximation is obtained by finding the minimizer to our proposed weighted optimization problem where the weights are chosen to be the uniform norms of the wavelet basis functions. We illustrate the effectiveness of this method by solving the image inpainting and image denoising problems. The constraints of the optimization problem are either a set of subsamples in the case of the inpainting problem or a set of noisy observations in the case of the denoising problem.",
    "tags": [],
    "title": "SIAM CSE 2019 Rom Minisymposium",
    "type": "talk"
  },
  {
    "authors": null,
    "categories": null,
    "content": "",
    "date": 1541693394,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1541693394,
    "objectID": "0156f00a03f247c62282583661cc8371",
    "permalink": "https://joedaws.github.io/project/spectral-clustering-sns-target-data/",
    "publishdate": "2018-11-08T11:09:54-05:00",
    "relpermalink": "/project/spectral-clustering-sns-target-data/",
    "section": "project",
    "summary": "The spallation neutron source (SNS) is a consumable component which wears out due to radiation damage and cavitation erosion. We propose a spectral clustering based method to detect damage in the target.",
    "tags": [
      "clustering",
      "data-analysis"
    ],
    "title": "Spectral Clustering SNS Target Data",
    "type": "project"
  },
  {
    "authors": [
      "Joseph Daws",
      "Armenak Petrosyan",
      "Hoang Tran",
      "Clayton Webster"
    ],
    "categories": null,
    "content": "",
    "date": 1541690015,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1541690015,
    "objectID": "52a989cbea29b5ac2cf3fd43ea5c9361",
    "permalink": "https://joedaws.github.io/publication/weighted-l1-minimization-and-closed-trees/",
    "publishdate": "2018-11-08T10:13:35-05:00",
    "relpermalink": "/publication/weighted-l1-minimization-and-closed-trees/",
    "section": "publication",
    "summary": "In this work we propose a convex optimization approach, based on weighted l1-regularization, for reconstructing sparse wavelet representations. We show our proposed optimiza- tion problem is effective for solving the signal/image inpainting and denoising problems. The wavelet coefficients associated to the functional representation of the object of interest are obtained as the solution to this optimization problem constrained by either subsamples or noisy observations in the case of inpainting or denoising, respectively. We show that by choosing the weights to be the uniform norms of the wavelet functions the support of the recovered vector of coefficients forms a particular kind of index set, a closed tree. This kind of index set is consistent with the behavior of many real-world signals and images and therefore our approach applies to a wide class of signals. Furthermore, we illustrate the effectiveness of the proposed convex optimization problem by providing numerical examples using both orthonormal wavelets and a frame of wavelets. In addition, we provide analysis to show that the sample complexity associated with the weighted l1-regularized problem is smaller than the sample complexity of the l1-regularized problem.",
    "tags": [
      "image-processing",
      "optimization"
    ],
    "title": "Weighted L1 Minimization and Closed Trees",
    "type": "publication"
  },
  {
    "authors": [
      "Joseph Daws"
    ],
    "categories": [
      "Neural-Networks"
    ],
    "content": " Towards a mathematical understanding of using neural networks for function approximation.  An artificial neural network (ANN) is a computational framework inspired by the interactions of neurons in the brain.\n Recently many difficult problems have been effectively solved using algorithms and mathematical models which iteratively improve their performance based on data rather than explicitly finding a solution. Such methods, which primarily rely on data rather than explicit solution techniques, can be grouped together under the umbrella of Machine Learning. The field of machine learning is rapidly exanding and new results often draw on a diverse background of mathematical, statistical, and even physics based methods. However, many of its most dramatic successes involve neural networks. Some examples I find interesting are:\n Image Classification \u0026ndash; A neural network is trained to classify handwritten numbers using a dataset of pre-labelled images of handwritten numbers. It is even able to classify images of digits not contained in the training set. Artificial intelligence \u0026ndash; Move prediction and selection for the game \u0026ldquo;Go\u0026rdquo; by a neural network. This network was able to beat a top level human player. Reinforced Learning \u0026ndash; A robotic hand controlled by a neural network is trained to manipulate a cube to requested positions. The network uses data from a camera to assess the current state of the cube and choose actions for the hand to perform in order to rotate it to a desired position.  All three of these examples employ a neural network to solve a complex problem.\nThere are many fundamental theoretical questions that remain (partially) unanswered about ANNs, e.g.,\n All networks require some training, a process by which pairs of inputs and outputs are used to set the internal parameters of the model. A popular strategy for training is accomplished by stochastic gradient descent (SDG). The convergence of this optimization problem is not well quantified or well understood. Given a network with a particular architecture, how does one determine the class of functions it can express? Moreover, even if optimal parameters can be shown to exist, are they easy to learn? Under what conditions will a network generalize well to inputs and outputs not contained in the training set? Some networks experience very bad overfitting and do not make good predictions for new incoming data.  In order to make sense of any of these theoretical considerations in a mathematical context it is useful to characterize ANN\u0026rsquo;s as both functions and graphs. In this post we will consider designing a network to approximate a given analytic function $f$. As a function, the network can be interpreted as a mapping from $\\mathbb{R}^d$ to $\\mathbb{R}^k$ where $d$ is the number of inputs to the network and $k$ is the number of ouputs. Let $$u_{NN}:\\mathbb{R}^d \\rightarrow \\mathbb{R}^k$$ be the functional interpretation of the neural network approximating an analytic function $u$. The function $u_{NN}$ is computed by a combining a series computations performed by constituent functions. The interactions between the constituent functions that make up the network can be described by a directed acylic graph. The associated graph can be used to analyze the complexity of the network defined as the number of trainable parameters.\nSimply put, a neural network can be characterized as a function consisting of compositions and linear combinations of constituent functions called neurons which perform a computation of the form $$ \\sigma \\left( \\sum_{i=1}^k w_i \\cdot x_i + b \\right).$$\n $x_i$ \u0026ndash; The inputs for the computation. They could be the result of computations performed by other neurons or raw input. $w_i$ \u0026ndash; The weight associated with the incoming data $x_i$. $\\sigma$ \u0026ndash; A nonlinear function called the activation function. $b$ \u0026ndash; A number called the bias of the neuron.  Nonlinear functions and graphs Below is a simple example showing the dual characterization of a single hidden layer ANN.\nA single hidden layer network with three neurons.\nThis graph depicts how each of the constituent computations are used to form the final output of the network. Here we call the function induced by the network $f$. The function $f$ maps the input $x$ to the output $f(x)$ and is computed as a linear combination of the computations performed in the hidden layer. One can also write $f$ so that it is easily recognized as a funciton.\nThe network as a function.\nThis representation makes explicit the mapping how $f$ maps the input $x$ to the output $f(x)$. It also shows how the mapping induced by the network depends entirely on the choice of the weights, biases and the activation function. Choosing these parameters directly determines how well the network $f$ performs its task.\nSuppose that we wish to design a network for function approximation, i.e., one is tasked to choose the parameters of $f$ so that it produces the same input\u0026ndash;output pairs as a function $g:\\mathbb{R} \\rightarrow \\mathbb{R}$. The problem of finding an optimal approximation of $g$ is then finding the optimal parameters for $f$ so that the quantity $error$ is as small as possible where $$ error = ||f - g||$$ for an appropriately chosen norm $|| \\cdot ||$. Notice that the approximation power of $f$ also depends on the choice of $\\sigma$. It is possible to consider $\\sigma$ as a parameter, but there are several established choices for $\\sigma$ that are widely used, see e.g. activation functions. Here I will consider using the so called Rectified Linear Unit (ReLU) activation function.\nThe ReLU actication function is a piecewise linear function.\nThe network $f$ is a very simple network. Networks with a single hidden layer are called shallow networks and in practice shallow networks have been shown to have limited usage. In fact, it is known that deep networks, i.e. networks with more than one hidden layer, are more expressive than shallow in the sense that they require less complex structure in order to express a given funciton. See the work of Mhaskar.\nThe architecture of a network depends on its application In practice, the design of neural networks is linked to their desired application. For instance, neural networks used for image classification typically involve convolutional kernels which extract local features. To my knowledge there is no clear theoretical justification of this choice of architecture. However, since there are successful image processing methods which exploit local and global image information simultaneously in a similar way to convolutional layers in a neural network, .e.g., LDMM and NONLOCAL MEANS.\nWhen designing a network to approximate a function it is therefore reasonable for the architecture of a proposed network to depend on a particular function approximation scheme or at least be inspired by an approximation scheme. This choice also allows one to take advantage of the large body of theoretical work describing methods to approximate functions (of various levels of smoothness and dimension). I\u0026rsquo;m currently analyzing a network based on polynomial approximation of high-dimensional functions as well as implementing one.\nCheck this post in the near future for some preliminary numerical results of this network as well as some theoretical estimates of its expressive power.\n",
    "date": 1541622218,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1541622218,
    "objectID": "eb93e03777b3eb1a2f9a34fdbe58ddf9",
    "permalink": "https://joedaws.github.io/post/designing-neural-networks/",
    "publishdate": "2018-11-07T15:23:38-05:00",
    "relpermalink": "/post/designing-neural-networks/",
    "section": "post",
    "summary": "Towards a mathematical understanding of using neural networks for function approximation.  An artificial neural network (ANN) is a computational framework inspired by the interactions of neurons in the brain.\n Recently many difficult problems have been effectively solved using algorithms and mathematical models which iteratively improve their performance based on data rather than explicitly finding a solution. Such methods, which primarily rely on data rather than explicit solution techniques, can be grouped together under the umbrella of Machine Learning.",
    "tags": [
      "machine-learning",
      "neural-networks"
    ],
    "title": "Designing Neural Networks",
    "type": "post"
  },
  {
    "authors": [
      "Joseph Daws"
    ],
    "categories": null,
    "content": "",
    "date": 1541613748,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1541613748,
    "objectID": "21b08b2b8290943ef989744e1cfbe5f5",
    "permalink": "https://joedaws.github.io/talk/imi-graduate-student-mini-conf/",
    "publishdate": "2018-11-07T13:02:28-05:00",
    "relpermalink": "/talk/imi-graduate-student-mini-conf/",
    "section": "talk",
    "summary": "In this work we propose a compressed sensing approach for the reconstruction of functions or images represented by their expansion in a wavelet basis, from only a small number of samples. The success of applying wavelet representations for image reconstruction and compression has inspired many sparse recovery techniques. However, these approaches can be improved by exploiting the connections between different levels of wavelet coefficients in the hierarchical wavelet basis to find significant coefficients. In particular, we show that the important wavelet coefficients for a certain class of functions and images are concentrated on a lower set (i.e., a downward closed tree), and present a weighted ℓ1 minimization technique which takes advantage of this fact in order to reduce the overall complexity of our proposed compressed sensing recovery. Following some of the results in our previous effort we also present theoretical estimates related to the sampling complexity of our scheme as compared to unweighted ℓ1 minimization. Several numerical examples are provided to show the effectiveness of this weighted ℓ1 minimization scheme for solving the image inpainting problem as compared to several other established solution techniques.",
    "tags": [],
    "title": "IMI Graduate Student Mini Conference",
    "type": "talk"
  },
  {
    "authors": null,
    "categories": null,
    "content": " Weighted $\\ell_1$-minimization In an upcoming work a convex optimization approach, based on weighted $\\ell_1$-regularization, is proposed for reconstructing sparse wavelet representations. We show our proposed optimization problem is effective for solving the signal/image inpainting and denoising problems. We take the funcational representation of an image or signal to be $$ f(y) = \\sum_{j \\in \\mathcal{J}} c_j \\Psi_j(y) . $$ The wavelet coefficients $c = (c_j)_{j \\in \\mathcal{J}}$ associated to the functional representation of the object of interest are obtained as minizers of\n$$ \\min_{c \\in \\mathbb{R}^N} \\lVert c \\rVert_{\\omega,1} + \\lVert Ac - f \\rVert_2, $$ where $f \\in \\mathbb{R}$ is either a vector $m$ subsamples or $m$ noisy observations.\nWe show that by choosing the weights to be the uniform norms of the wavelet functions, i.e., the $L^\\infty$-norm of $\\Psi_j$, the support of the recovered vector of coefficients forms a particular kind of index set, a closed tree.\nAn example of a closed tree. The solid edges form the closed tree.\nThis kind of index set is consistent with the behavior of many real-world signals and images and therefore our approach applies to a wide class of signals. Furthermore, the numerical examples below show the effectiveness of weighted $\\ell_1$-minimization. In addition, we have shown that the sample complexity associated with the weighted $\\ell_1$-regularized problem is smaller than the sample complexity of the unweighted problem. This analysis will appear in an upcoming paper.\nImage Inpainting Compare the recovery of a picture of flamingos using unweighted and weighted $\\ell_1$-minimization from a random sample of $8\\%$ of the pixels. The original image is a $972 \\times 1296$ pixel image. We used the Daubechies 3 db3 basis.\nImage De-noising The weighted $\\ell_1$-minimization problem can also be used to de-noise an image:\n",
    "date": 1541429312,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1541429312,
    "objectID": "e524fab433beb5ba38398f147447384b",
    "permalink": "https://joedaws.github.io/project/image-inpainting/",
    "publishdate": "2018-11-05T09:48:32-05:00",
    "relpermalink": "/project/image-inpainting/",
    "section": "project",
    "summary": "Using weighted convex minimization to solve the image inpainting and de-noising problems.",
    "tags": [
      "optimization",
      "convex-optimization",
      "image-processing"
    ],
    "title": "Image Processing and Optimization",
    "type": "project"
  }
]