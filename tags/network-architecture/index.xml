<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>network-architecture on Joseph Daws</title>
    <link>https://joedaws.github.io/tags/network-architecture/</link>
    <description>Recent content in network-architecture on Joseph Daws</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Mon, 14 Jan 2019 11:36:57 -0500</lastBuildDate>
    
	<atom:link href="https://joedaws.github.io/tags/network-architecture/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Polynomials, Binary Trees and Gradient Descent</title>
      <link>https://joedaws.github.io/post/binary-tree-network/</link>
      <pubDate>Mon, 14 Jan 2019 11:36:57 -0500</pubDate>
      
      <guid>https://joedaws.github.io/post/binary-tree-network/</guid>
      <description>Nonlinear Approximation As described in my previous post Neural networks have emerged as a very powerful tool for constructing nonlinear functions in high-dimensions. They are known to be very expressive in the sense that the class of functions they can approximate is very wide. However, in order to cosntruct an approximation of a given function a very large number of network parameters must be determined. This is espcially true for Deep Neural Networks (DNNs).</description>
    </item>
    
  </channel>
</rss>